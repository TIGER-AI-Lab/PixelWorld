<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="PixelWorld: Towards Perceiving Everything as Pixels">
    <meta property="og:title" content="PixelWorld: Towards Perceiving Everything as Pixels" />
    <meta property="og:description" content="We propose to perceive everything as pixles for unified perception" />
    <meta property="og:url" content="https://github.com/TIGER-AI-Lab/PixelWorld" />
    <meta property="og:image" content="" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <title>PixelWorld</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            PixelWorld: Towards Perceiving Everything as Pixels
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Zhiheng Lyu,
                            </span>
                            <span class="author-block">
                                Xueguang Ma,
                            </span>
                            <span class="author-block">
                                Wenhu Chen
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- GitHub link -->
                                <span class="link-block">
                                    <a href="https://github.com/TIGER-AI-Lab/PixelWorld" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                    </a>
                                </span>

                                <!-- Paper link (placeholder) -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2501.19339" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                  <a href="https://huggingface.co/datasets/TIGER-Lab/PixelWorld" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        ü§ó
                                      </span>
                                      <span>PixelWorld Dataset</span>
                                  </a>
                                </span>
                            </div>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                            University of Waterloo, Vector Institute
                            </span>
                            <br>
                            <span class="author-block">
                                <small>
                                    zhiheng.lyu@uwaterloo.ca,
                                    wenhuchen@uwaterloo.ca
                                </small>
                            </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h1 class="title is-3">Abstract</h1>
                    <div class="content has-text-justified">
                        <p>
                            Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. ‚ÄúPerceive Everything as Pixels‚Äù (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models' performance. Our findings show that (1) PixelWorld outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="MethodOverview">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h1 class="title is-3">Method Overview: Perceive Everything as Pixels (PEAP)</h1>
              <div class="content">
                <p>
                  We propose to unify all modalities (text, tables, code, diagrams, images, etc.) as pixel inputs. This approach, called <strong>PEAP</strong>, is designed to better align with human perception and reduce the need for excessive pre-processing.
                </p>
                <p>
                  The diagram below (corresponding to <strong>Figure 1</strong> in the paper) illustrates the overall PEAP framework.
                </p>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
              <figure class="image">
                <img src="static/images/head_figure.jpg" alt="PEAP Overview Diagram">
                <figcaption>
                    <strong>Figure 1:</strong> PEAP framework: we investigate the possibility of perceiving everything as pixels. This framework aligns better with human perception, reducing the need for excessive pre-processing. Evaluated on our benchmark <em>PixelWorld</em>, PEAP boosts performance on multimodal tasks (e.g., websites, slides, documents) but struggles with complex, text-centric tasks (e.g., reasoning and coding). Larger models achieve better transferability between pixel- and token-based performance compared to smaller ones. We also observed that text and images exhibit similar attention patterns and reduced the overhead of model reasoning through patch pruning by PEAP-Fast.
                </figcaption>
              </figure>
              <!-- <p class="subtitle is-5">
                Figure 1: PEAP framework: we investigate the possibility of perceive everything as pixels. This framework aligns better with human perception reducing the need for excessive pre-processing.   Evaluated on our benchmark \textsc{PixelWorld}, \method boosts performance on multimodal tasks (e.g., websites, slides, documents) but struggles with complex, text-centric tasks (e.g., reasoning and coding). Larger models achieve better transferability between pixel- and token-based performance compared to smaller ones. We also observed that text and images exhibit similar attention patterns, and reduced the overhead of model reasoning through patch pruning by \method-Fast.
              </p> -->
            </div>
          </div>
        </div>
      </section>
      
      <section class="section hero is-light" id="DatasetSection">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h1 class="title is-3">Dataset</h1>
              <div class="content">
                <p>
                  We construct a comprehensive evaluation suite to systematically analyze and compare VLMs‚Äô performance across text-only, structured, and multimodal inputs. <strong>Table 1</strong> below (adapted from the original paper) summarizes the key datasets with their sizes, tasks, and modality transfer methods. These include established benchmarks such as GLUE, SuperGLUE, and MMLU-Pro for text-only tasks, TableBench for structured data, and MathVerse or SlidesVQA for inherently multimodal scenarios.
                </p>
                <p>
                  To create pixel-based input for text-only and structured data, we employ an image synthesis pipeline, which renders plain text or tabular data into images of varying size and font style. <strong>Figure 2</strong> (also from the original paper) illustrates an example of how an input prompt is transformed into a visually rendered image for model inference. This approach ensures that layout information is preserved, reducing potential OCR errors and aligning more closely with our ‚ÄúPerceive Everything as Pixels‚Äù (PEAP) paradigm.
                </p>
              </div>
            </div>
          </div>
      
          <!-- Figure 1: Dataset Composition Table -->
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
              <figure class="image">
                <img src="static/images/table1.jpg" alt="Dataset Composition Overview">
                <figcaption>
                  <strong>Table 1:</strong> Dataset Composition Overview
                </figcaption>
              </figure>
            </div>
          </div>

          <!-- Figure 2: Image Synthesis Example -->
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
              <figure class="image">
                <img src="static/images/example_input_2.png" alt="Synthesized Input Example">
                <figcaption>
                  <strong>Figure 2:</strong> Example of Synthesized Input
                </figcaption>
              </figure>
            </div>
          </div>

        </div>
      </section>
      

      <section class="section" id="EvaluationResults">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h1 class="title is-3">Evaluation Results</h1>
              <div class="content">
                <p>
                  We comprehensively evaluated PEAP on text-only, structured, and multimodal tasks. The key findings include:
                </p>
                <ul>
                  <li>PEAP outperforms token-based baselines on multimodal tasks by providing better disambiguation.</li>
                  <li>Significant performance degradation is observed in complex reasoning and coding tasks when processing pixel-based inputs.</li>
                  <li>Larger models can maintain strong performance under PEAP, whereas smaller models face more challenges.</li>
                  <!-- <li>The attention patterns of PEAP closely align with those of token-based inputs.</li>
                  <li>The proposed <strong>PEAP-Fast</strong> method significantly accelerates inference by exploiting spatial sparsity.</li> -->
                </ul>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
                <figure class="image">
                    <img src="static/images/plt_decompose.png" alt="Evaluation Results (Text-only)">
                    <figcaption>Evaluation Results (Text-only)</figcaption>
                  </figure>
                  
                  <figure class="image">
                    <img src="static/images/plt_decompose_sc32.png" alt="Evaluation Results (Structural)">
                    <figcaption>Evaluation Results (Structural)</figcaption>
                  </figure>
                  
                  <figure class="image">
                    <img src="static/images/plt_decompose_sc33.png" alt="Evaluation Results (Multimodal)">
                    <figcaption>Evaluation Results (Multimodal)</figcaption>
                  </figure>                  
              <!-- <p class="subtitle is-5">
                Figure 2, 3, 5: Performance Comparison
              </p> -->
            </div>
          </div>
        </div>
      </section>

      <section class="section hero is-light" id="AttentionVisualization">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h1 class="title is-3">Attention Visualization</h1>
              <div class="content">
                <p>
                  To gain insight into how the model processes pixel inputs, we visualized the attention maps of Qwen2VL-7B‚Äôs final layer for both token-based and pixel-based inference. The results, shown below (corresponding to <strong>Figure 3</strong> in the paper), reveal that the attention distribution is largely consistent across modalities.
                </p>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
              <figure class="image">
                <img src="static/images/attention_heatmap.jpg" alt="Attention Heatmap">
              </figure>
              <p class="subtitle is-5">
                <strong>Figure 3</strong>: Attention Heatmap Comparison
              </p>
            </div>
          </div>
        </div>
      </section>

      <section class="section" id="EfficiencyImprovements">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-10">
              <h1 class="title is-3">Efficiency Improvements with PEAP-Fast</h1>
              <div class="content">
                <p>
                  Pixel-based input usually incurs higher computational overhead due to redundant blank patches. To mitigate this, we introduce <strong>PEAP-Fast</strong>, a sparsification algorithm that prunes non-informative regions, thereby reducing inference latency significantly (Table 3) while preserving model accuracy (Table 2).
                </p>
              </div>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column is-8 has-text-centered">
                <div style="display: flex; justify-content: space-between;">
                  <figure class="image" style="width: 50%;">
                      <img src="static/images/table_pref.jpg" alt="Efficiency Improvements" style="width: 100%;">
                      <figcaption>
                          <strong>Table 2:</strong> Impact of PEAP-Fast on Model Accuracy in SuperGLUE subsets.
                      </figcaption>
                  </figure>
                  <figure class="image" style="width: 50%;">
                      <img src="static/images/table_speed.jpg" alt="Efficiency Improvements" style="width: 100%;">
                      <figcaption>
                          <strong>Table 3:</strong> Reduction in Inference Latency with PEAP-Fast.
                      </figcaption>
                  </figure>
            </div>            
              <figure class="image">
                <img src="static/images/heatmap_sparse.jpg" alt="Efficiency Improvements">
                <figcaption>
                    <strong>Figure 4:</strong> Efficiency Improvements with PEAP-Fast on the Heatmap.
                </figcaption>
              </figure>
              <!-- <p class="subtitle is-5">
                Figure 9: Efficiency Improvements with PEAP-Fast
              </p> -->
            </div>
          </div>
        </div>
      </section>
      

    <!-- BibTeX citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Reference</h2>
            Please kindly cite our paper if you use our code or results:
            <pre><code>@article{lyu2024pixelworld,
    title={PixelWorld: Towards Perceiving Everything as Pixels},
    author={Lyu, Zhiheng and Ma, Xueguang and Chen, Wenhu},
    year={2025},
    eprint={2501.19339},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={http://arxiv.org/abs/2501.19339},
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content has-text-centered">
                        <p>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
</html>
